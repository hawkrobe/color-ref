---
title: "R Notebook"
output: html_notebook
---

# Import libraries

```{r}
library(tidyverse)
library(knitr)
library(scales)
library(xtable)
library(GGally)
library(ggrepel)

source('helpers.R')
```

# Import raw data & implement exclusion criteria

```{r}
d.raw = read_csv('../data/norming/color_raw.csv', show_col_types=F)
```

Only include participants who completed exactly 89 color-picker trials (all 80 words + 9 practice/catch trials)

```{r}
complete_ids = d.raw %>%
  group_by(participantID) %>%
  tally() %>%
  filter(n == 89) %>% 
  pull(participantID)
```

Check how many people were removed on catch trials vs. just dropped out

```{r results='asis'}
d.raw.final_trial <- d.raw %>%
  group_by(participantID) %>%
  filter(trial_index == last(trial_index)) %>%
  group_by(condition) %>%
  tally() %>%
  spread(condition, n)

cat('we lost', d.raw.final_trial$block1_catch_trial, 'on the first block catch trial,\n',
    d.raw.final_trial$block2_catch_trial, 'on the second block catch trial,\n', 
    d.raw.final_trial$catch_trial_color_trials, 'on the practice trial catch\n for a total of',
    d.raw.final_trial$catch_trial_color_trials
      + d.raw.final_trial$block1_catch_trial
      + d.raw.final_trial$block2_catch_trial)
```

Exclude based on colorblindness & RT

```{r}
# plate2 => normal = ['8' => key 56], red-green = ['3' => key 51]
# plate4 => normal = ['5' => key 53], red-green = ['2' => key 50]
# plate5 => normal = ['3' => key 51], red-green = ['5' => key 53]
colorblind_ids = read_csv('../data/norming/colorblindness.csv', show_col_types=F) %>%
  filter(participantID %in% complete_ids) %>%
  filter(case_when(stimulus == 2 ~ key_press != 56 & key_press != 51,
                   stimulus == 4 ~ key_press != 53 & key_press != 50,
                   stimulus == 5 ~ key_press != 51 & key_press != 53)) %>%
  pull(participantID) %>%
  unique()

cat('removed another', colorblind_ids %>% length(), 'for colorblindness') 
```

```{r}
low_rt_ids <- d.raw %>%
  filter(participantID %in% complete_ids) %>%
  filter(!(participantID %in% colorblind_ids)) %>%
  filter(condition %in% c('block1_target_trial', 'block2_target_trial')) %>%
  group_by(participantID) %>%
  arrange(participantID,trial_index) %>%
  mutate(low_rt = rt < 1000) %>%
  summarize(m = mean(low_rt)) %>%
  filter(m > 0.25) %>%
  pull(participantID) %>%
  unique()

cat('removed another', low_rt_ids %>% length(), 'for consistently <1s rts') 

response_streak_ids <- d.raw %>%
  filter(participantID %in% complete_ids) %>%
  filter(!(participantID %in% colorblind_ids)) %>%
  filter(!(participantID %in% low_rt_ids)) %>%
  filter(condition %in% c('block1_target_trial', 'block2_target_trial')) %>%
  group_by(participantID) %>%
  arrange(participantID,trial_index) %>%
  mutate(sameResponseAsPreviousTrial = lag(button_pressed) == button_pressed,
         threeInARow = paste0(lag(sameResponseAsPreviousTrial),
                              sameResponseAsPreviousTrial,
                              lead(sameResponseAsPreviousTrial)) == 'TRUETRUETRUE',
         m = mean(sameResponseAsPreviousTrial, na.rm = T)) %>%
  filter(threeInARow) %>%
  pull(participantID) %>%
  unique()

cat('removed another', response_streak_ids %>% length(), 
    'for same response more than 3 in a row') 
```

Filter & check how balanced across words this leaves us

```{r}
d <- d.raw %>%
  filter(participantID %in% complete_ids) %>%
  filter(!(participantID %in% colorblind_ids)) %>%
  filter(!(participantID %in% response_streak_ids)) %>%
  filter(!(participantID %in% low_rt_ids)) %>%
  filter(condition %in% c('block1_target_trial', 'block2_target_trial'))

d %>%
  group_by(wordSetID, participantID) %>%
  tally() %>%
  group_by(wordSetID) %>%
  tally()

cat('including', length(unique(d$participantID)), 'out of the', 
    length(unique(d.raw$participantID)), 'we recruited')
```

Save out cleaned data for python scripts:

```{r}
write_csv(d, "../data/norming/color_preprocessed.csv")
```

```{r}
# Uncomment to re-run python script; change path to your python binary / conda environment
# library(reticulate)
# use_python("/Users/roberthawkins/miniconda3/bin/python3", required = T)
# use_condaenv(condaenv = 'base')
# py_run_file("experiment1.py")
```

# Pre-process

Pre-compute some variance metrics and pull in word norms

```{r}
wordset.assignments <- d %>%
  group_by(word) %>%
  summarize(wordSetID = as.integer(mean(wordSetID)))

d.color <- d %>%
  cbind(c(.$response_r/255, .$response_g/255, .$response_b/255) %>%
        matrix(ncol = 3) %>%
        convertColor(from = 'sRGB', to = 'Lab') %>%
        data.frame()) %>%
  select(participantID, condition, word, L, a, b)

# get imageability and concreteness
glasgow <- getGlasgow()

# get entropies
entropies <- d %>%
  group_by(word, button_pressed) %>%
  tally() %>%
  ungroup() %>%
  complete(button_pressed, word, fill = list(n = 0)) %>%
  group_by(word) %>%
  summarize(entropy = entropy::entropy(n, method = 'SG', unit="log2"))
  
# get mean pairwise distances across entire population
deltae.pop <- d.color %>%
  left_join(d.color, by = c('word')) %>%
  filter(participantID.x != participantID.y) %>%
  mutate(deltae.pop = spacesXYZ::DeltaE(matrix(c(L.x, a.x, b.x), ncol = 3), 
                                        matrix(c(L.y, a.y, b.y), ncol = 3), 
                                        metric = '2000')) %>%
  group_by(word) %>%
  summarize(deltae.pop = mean(deltae.pop))

means.pop <- d.color %>%
  group_by(word) %>%
  summarize(meanL = mean(L), meanA = mean(a), meanB = mean(b), 
            stdL = sd(L), stdA = sd(a), stdB = sd(b))
```

Consolidate everything in a single dataframe 

```{r}
dCompiled <- entropies %>%
  left_join(deltae.pop) %>%
  left_join(glasgow) %>%
  left_join(wordset.assignments) %>%
  left_join(means.pop) %>%
  filter(!is.na(imageability)) 
```

# Analysis 1: To what extent does concreteness/imageability predict variability

## Visualize pairwise relationships between metrics & test correlations

Look at pairwise correlations

```{r}
dCompiled %>% 
  select(entropy, deltae.pop, concreteness, imageability) %>%
  mutate(concreteness = as.numeric(scale(concreteness)),
         imageability = as.numeric(scale(imageability)),
         deltae.pop = as.numeric(scale(deltae.pop)),
         entropy = as.numeric(scale(entropy))) %>%
  GGally::ggpairs(lower = list(continuous = "smooth"), 
                  upper = list(continuous = wrap('cor', method = "spearman")), 
                  progress = F) + 
  ggthemes::theme_few()

ggsave('../analyses/concreteness_imageability_pairs.pdf', height = 6, width = 7)
```

## regression model for predicting entropy 


```{r}
dCompiled %>%
  lm(scale(entropy) ~ imageability + concreteness, data =.) %>%
  summary()
```

## regression model predicting deltaE 

Concreteness has small non-redundent effect when predicting delta-E. 

```{r}
dCompiled %>% 
  lm(deltae.pop ~ imageability + concreteness, data = .) %>%
  summary()
```

Note that this relationship flips if we include the color words from the practice trials (i.e. 'blue', 'red', 'green'.) 
In other words, concreteness becomes significant when predicting entropy but *not* delta-e. 
Why? One observation is that these color words tend to have high imageability but low concreteness and are at the low end of the entropy/delta-e scales.
The measures of variation pull apart because of the slight warping of the space of munsell chips (e.g. with more blue chips than yellow chips, meaning lower delta-e than entropy) so these additional data points must interact with that different between measures.

## Analysis 2: Intra-participant variability

```{r}
d.intra <- d.color %>%
  separate(condition, into = c('block', 'trialtype')) %>%
  filter(block %in% c('block1', 'block2'), trialtype == 'target') %>%
  group_by(participantID, word) %>%
  pivot_wider(names_from = block, values_from = c('L', 'a', 'b')) %>%
  mutate(block1_block2_dist = spacesXYZ::DeltaE(matrix(c(L_block1, a_block1, b_block1), ncol = 3), 
                                        matrix(c(L_block2, a_block2, b_block2), ncol = 3), 
                                        metric = '2000')) 
```

```{r}
# get details of regression corresponding to figure 2B
d.intra.collapsed <- d.intra %>%
  group_by(word) %>%
  tidyboot::tidyboot_mean(block1_block2_dist, nboot = 100) %>%
  right_join(dCompiled %>% select(word,deltae.pop)) %>%
  rename(internal = empirical_stat, population = deltae.pop)
  
# prior dist = average block1 delta E for each word (population variability)
# empirical_stat= average of individuals' block1-block2 delta E for each word (internal variability)
cor.test(d.intra.collapsed$internal, d.intra.collapsed$population)

d.intra.collapsed %>%
  lm(population ~ internal, data=.) %>%
  summary()
```


# Average block1-block2 deltaE's for each word and plot

```{r}
d.intra %>%
  group_by(word) %>%
  tidyboot::tidyboot_mean(block1_block2_dist, nboot = 100) %>%
  rename(deltae.internal = empirical_stat) %>%
  left_join(deltae.pop) %>%
  ggplot(aes(x = deltae.pop, y = deltae.internal)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, linetype = 'dotted') +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, alpha = 0.25) +
  geom_smooth(method = 'lm', formula = y ~ poly(x,1), color = 'red') +
  labs(x="Average delta E between different participants", 
       y= "Average delta E within participant") +
  xlim(0, 43) +
  ylim(0, 43) +
  ggthemes::theme_few() +
  theme(aspect.ratio = 1)

ggsave("./experiment1/figures/block1-block2-deltaE.pdf", width=4, height=3)
```

# Analysis 3: Compare expectations with ground truth metrics

## Compare expectation (average slider response) to actual percent agreement

```{r}
# start with raw expectation ratings
expectations <- read_csv('../data/norming/expectations.csv') %>%
  filter(participantID %in% d$participantID)  %>%
  select(participantID, word, sliderResponse) %>%
  
  # merge in participant's own response on trial where they provided rating
  left_join(d %>% 
              filter(condition == 'block2_target_trial') %>% 
              select(word, button_pressed, participantID, condition)) %>%
  select(-condition) %>%
    
  # merge in other participants' responses (from both blocks)
  left_join(d %>% select(word, button_pressed, participantID, condition), by = c('word')) %>%
  filter(participantID.x != participantID.y) %>%
  mutate(match = button_pressed.x == button_pressed.y)

expectations.boot <- expectations %>%
  
  # Compute proportion of matches for each participant-word pair
  # (adding pseudo-count to regularize & avoid zeros)
  group_by(word, participantID.x) %>%
  summarize(sliderResponse = mean(sliderResponse),
            button_response = mean(button_pressed.x),
            match = (mean(c(match, 1)))) %>%
  
  # z-score slider responses within participant 
  # (i.e. expectation for this word relative to others)
  group_by(participantID.x) %>%
  mutate(expected = scale(sliderResponse),
         true_log_odds = log(match / (1-match)),
         true_log_prob = log(match)) %>%
  gather(metric, value, expected, true_log_odds, true_log_prob) %>%
    
  # bootstrap error bars
  group_by(word, metric) %>%
  tidyboot::tidyboot_mean(value, nboot = 1000, na.rm = T) %>%
  select(-mean, -n) %>%
  pivot_wider(names_from = metric, values_from = c('empirical_stat', 'ci_lower', 'ci_upper'))
```


```{r}
expectations.boot %>%
  ggplot(aes(x = empirical_stat_expected, y = empirical_stat_true_log_odds)) +
  geom_point() +
  geom_errorbarh(aes(xmax = ci_upper_expected, xmin = ci_lower_expected), alpha = 0.25) +
  geom_errorbar(aes(ymax = ci_upper_true_log_odds, ymin = ci_lower_true_log_odds), alpha = 0.25) +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 1), color = 'red') +
  labs(x = 'expected agreement (scaled)', y = 'actual agreement (log odds)') +
  xlim(-1.5, 1.7) +
  ggthemes::theme_few() +
  theme(aspect.ratio = 1)

ggsave(here('analyses/python/experiment1/figures/expectations.pdf'), height = 3, width = 3, units = 'in')
```


# get summary of regression for figure 2C

```{r}
cor.test(tmp$empirical_stat_actual, tmp$empirical_stat_expected,  method = 'pearson')

# might worry about log transform, but rank correlation on raw (untransformed) agreement probability is similarly high
cor.test(exp(tmp$empirical_stat_actual_alt), tmp$empirical_stat_expected, method = 'spearman')
```

# Supplemental

Visualize color grids

```{r}
d %>%
  mutate(hex = rgb(response_r/255, response_g/255, response_b/255)) %>%
  group_by(word) %>%
  do(ggsave(filename = paste0('color_grids/', .$word, '.png'), device = 'png', 
            plot = show_col(.$hex, labels = F)))
```

Visualize entropies

```{r}
ordered <- dCompiled[order( dCompiled$entropy ),]
barplot(ordered$entropy, main="Entropy", xlab="words")
```

Plot DeltaE for all words

```{r}
dIntraparticipant %>%
  mutate(word = factor(word)) %>%
  ggplot(aes(x = fct_reorder(word, block1_block2_dist), y = block1_block2_dist)) + 
    geom_boxplot(width = 1) +
    labs(y="Delta E (CIE2000)", x = "Word") + 
    guides(x = guide_axis(n.dodge = 1, angle = 90, check.overlap = T)) +
    ggtitle("Delta E between individual participants' block1 and block2 color response") +
    ggthemes::theme_few() +
    theme(aspect.ratio = 1/5) 
```

Experiment 1, analysis 1: Compare computing deltaE by pooling both blocks' responses (prior_dist) to python calculation over only block1

```{r}
compare <- d.color %>%
  left_join(d.color, by = c('word')) %>%
  filter(participantID.x != participantID.y) %>%
  mutate(condition = case_when(condition.x == 'block1_target_trial' & condition.y == 'block1_target_trial' ~ 'block1',
                               condition.x == 'block2_target_trial' & condition.y == 'block2_target_trial' ~ 'block2',
                               TRUE ~ 'crossblock')) %>%
  mutate(deltae.pop = spacesXYZ::DeltaE(matrix(c(L.x, a.x, b.x), ncol = 3), 
                                        matrix(c(L.y, a.y, b.y), ncol = 3), 
                                        metric = '2000')) %>%
  group_by(word, condition) %>%
  summarize(deltae.pop = mean(deltae.pop)) %>%
  spread(condition, deltae.pop) %>% 
  ungroup() %>% 
  summarize(b12 = cor(block1, block2), 
            b1c = cor(block1,`crossblock`), 
            b2c = cor(block2,`crossblock`))

# compare$diff <- abs(compare$bothBlocks - compare$block1)
# 
# ggplot(compare, aes(x=word, y= diff, label=word))+
#   geom_point(size = 1) +
#   geom_text(aes(label=word),hjust=0, vjust=0, size=3)
# 
# cor(compare$bothBlocks, compare$block1)
# sum(compare$diff > 1)

dCompare <- dCompiled %>%
  left_join(compare, by = c('word'))
```

Compare linear regression models for different deltaE calculations

```{r}
linearMod <- lm(bothBlocks ~ imageability + concreteness, data=dCompare)
summary(linearMod)

linearMod <- lm(block1 ~ imageability + concreteness, data=dCompare)
summary(linearMod)
```

Conclusion: calculating deltaE over both block1 and block2 responses (as done in R analyses) does not produce significantly different results than calculating deltaE over only block1 responses (as done in Python analyses)


## Compare expectation (average slider response) to DeltaE among participants' block 2 responses

```{r}
dDeltaE = read_csv('./experiment1/analysis3/sorted-deltaE-block2.csv') 

# zscoring:
deltaE <- dDeltaE[order(dDeltaE$word),] %>%
  mutate(zscore = (deltaE - mean(deltaE))/sd(deltaE))
# averaging slider + zscoring

expectation <- aggregate(dExpectation[, 2], list(dExpectation$word), mean) %>%
  rename(word = "Group.1", avgExpectation = sliderResponse) %>%
  mutate(zscore = (avgExpectation - mean(avgExpectation))/sd(avgExpectation))

# plot
scatter.smooth(x=expectation$zscore, y=deltaE$zscore, xlab="Average expecation rating", ylab="Average Delta E among block2 repsonses", main="Expected agreement vs. average Delta E")

# correlation
cor.test(expectation$zscore, deltaE$zscore)
```


## Compare expectation (average slider response) to entropy

```{r}
# load and format entropy data
dEntropy = read_csv('./experiment1/analysis1/sorted-entropy-both.csv')

entropy <- dEntropy[order(dEntropy$word),] %>%
  mutate(entropy = as.numeric(entropy)) %>%
  mutate(zscore = (entropy - mean(entropy))/sd(entropy))
  
scatter.smooth(x=expectation$zscore, y=entropy$entropy, xlab = "Entropy of participants' color associations for all words", ylab = "Average % of population expected to share one's color association")

# correlation
cor.test(expectation$zscore, entropy$zscore)
```

## Expectation ratings vs. ground truth (calculated as percentage of total response that agreed with each participant's response)

The following analyses show that the ground truth agreements are much lower than the slider responses for expected agreement. Plotting the distributions of the expected and actual agreements for the 10 words with the least entropy in their prior distributions (i.e. most agreement), shows that only the words whose distributions would be expected to center around yellows, have an actual agreement that is somewhat reflective of their concreteness and expectation ratings. Meanwhile, the words whose response distributions would be expected to center on reds ("tomato", "fire", "apple") and greens ("spinach", "vegetable", "tree") have actual agreements that are much lower than expected. In the context of the munsell space that participants selected from--one which has a larger variety of red and green color choices, but very few yellow ones--this discrepancy suggests that when people rate their expected agreement they're considering the space of all reds, greens, etc. as "sharing" their color association (vs. the particular color they picked), but in reality words like "tomato" and "fire" and "vegetable" have many of different red or green responses that deflate the agreement.
```{r}
library("ggpubr")
ggboxplot(dExpectation, y = "sliderResponse", main= "Expected Agreement by Condition",
          color = "condition", order = c("concrete", "abstract"), ylim = c(0, 100),
          ylab = "% of population expected to share association") +
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), axis.title.x=element_blank())
```

### Ground truth boxplot
```{r}
ggboxplot(dTruth, y = "groundTruth", main= "Actual Agreement by Condition",
          color = "condition", order = c("concrete", "abstract"), ylim = c(0, 100),
          ylab = "% of population actually sharing color association") +
    theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), axis.title.x=element_blank())
```

### plotting expected and actual agreement for top 10 words individually
```{r}
# define "expected" and "actual" condition columns
expect <- rep(c("expected"),each=981)
actual <- rep(c("actual"),each=981)

# prepare datasets to be combined
selectedExpectation <- filter(dExpectation, word %in% c("lemon", "sun", "tomato", "fire", "apple", "spinach", "vegetable", "tree", "daffodil")) %>%
  subset(select=-condition) %>%
  rename(value = sliderResponse) %>%
  cbind(condition = expect)
  
selectedActual <- filter(dTruth, word %in% c("lemon", "sun", "tomato", "fire", "apple", "spinach", "vegetable", "tree", "daffodil")) %>%
  subset(select=-condition) %>%
  rename(value = groundTruth) %>%
  cbind(condition = actual)
# append datasets
both <- rbind(selectedActual, selectedExpectation)

# plot
ggplot(both, aes(x=word, y=value, fill=condition)) +
  geom_boxplot() +
  ggtitle("Actual vs. Expected Agreement") +
  labs(y="% agreement", x = "Word") +
  # facet_wrap(~condition) +
  coord_cartesian(ylim = c(0, 100)) +
  theme(axis.text.x = element_text(size = 10, angle = 45), axis.title=element_text(size=12))

```
## Logistic regression
```{r}
# takes a second to run
require(lme4)

m <- glmer(matchYN ~ aID1Expectation + (1 | aID1), data = dLogistic)
# print the mod results without correlations among fixed effects
print(m)
```

